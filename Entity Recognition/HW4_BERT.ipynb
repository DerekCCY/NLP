{"cells":[{"cell_type":"markdown","metadata":{"id":"TJIAQTsAzkZ0"},"source":["# HW4: Fine-tuning BERT for entity labeling\n","This notebook contains starter code for finetuning a BERT-style model for the task of entity recognition. It has minimal text so you can easily copy it to **handin.py** when you submit.  Please read all the comments in the code as they contain important information."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"UjihAVr90bDo","executionInfo":{"status":"ok","timestamp":1733448108640,"user_tz":300,"elapsed":7699,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# This code block just contains standard setup code for running in Python\n","import time\n","\n","# PyTorch imports\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Subset #random_split\n","import numpy as np\n","\n","import csv\n","import os\n","# Fix the random seed(s) for reproducability\n","torch.random.manual_seed(8942764)\n","torch.cuda.manual_seed(8942764)\n","np.random.seed(8942764)\n","\n","# Please set your device by uncommenting the right version below\n","\n","# On Colab or on a machine with access to an Nvidia GPU use the following setting\n","#device = 'cuda:1'\n","device = 'cuda'\n","# if you have an Apple Silicon machine with a GPU, use the following setting\n","# this should about 3-4 times faster that running it on just CPU\n","# device = 'mps'\n","\n","# If you will use a cpu, this is the setting\n","# device = 'cpu'\n","\n","# Note that in handin.py these next two lines will need to be removed\n","# if you are going run this on your personal machine you will need to install\n","# these locally in the shell/terminal.\n","\n","#!pip install protobuf==3.20.2\n","#!pip install transformers\n","#!pip install datasets\n","#!pip install evaluate\n","#!pip install seqeval\n","\n","from transformers import AutoTokenizer, BertModel, DataCollatorForTokenClassification\n","\n","import evaluate"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"T2WWcTeDonhf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448111011,"user_tz":300,"elapsed":2379,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"e0ee6672-1527-43e1-e591-319902874263"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"duHZ1XZMoYkZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448111338,"user_tz":300,"elapsed":329,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"03e50cad-493f-4a0c-80ce-a2469632692e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Labels: ['I-Aquatic_animal', 'B-Deity', 'B-Mythological_king', 'I-Mythological_king', 'I-Cretaceous_dinosaur', 'B-Aquatic_animal', 'B-Aquatic_mammal', 'I-Goddess', 'I-Deity', 'B-Cretaceous_dinosaur', 'I-Aquatic_mammal', 'B-Goddess', 'O']\n","DatasetDict({\n","    train: Dataset({\n","        features: ['para_index', 'title', 'doc_id', 'content', 'page_id', 'id', 'tokens', 'ner_strings', 'ner_tags'],\n","        num_rows: 1749\n","    })\n","    dev: Dataset({\n","        features: ['para_index', 'title', 'doc_id', 'content', 'page_id', 'id', 'tokens', 'ner_strings', 'ner_tags'],\n","        num_rows: 150\n","    })\n","})\n"]}],"source":["# Load the dataset\n","from datasets import ClassLabel, Sequence, load_dataset\n","\n","data_splits = load_dataset('json', data_files={'train': '/content/drive/MyDrive/HW4/dinos_and_deities_train_bio.jsonl', 'dev': '/content/drive/MyDrive/HW4/dinos_and_deities_dev_bio_sm.jsonl'})\n","\n","label_names_fname = \"/content/drive/MyDrive/HW4/dinos_and_deities_train_bio.jsonl.labels\"\n","labels_int2str = []\n","with open(label_names_fname) as f:\n","    labels_int2str = f.read().split()\n","print(f\"Labels: {labels_int2str}\")\n","labels_str2int = {l: i for i, l in enumerate(labels_int2str)}\n","\n","data_splits.cast_column(\"ner_tags\", Sequence(ClassLabel(names=labels_int2str)))\n","print(data_splits)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gaqtJZZFmDMf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448113174,"user_tz":300,"elapsed":1838,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"8ed7de48-9a32-4f04-91c8-8304bd3e63fa"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["# initialize pretrained BERT tokenizer. This might take a while the first time it's run because the model needs to be downloaded.\n","# Note: if you change the BERT model later, don't forget to also change this!!\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-uv_urtjmQH2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448113174,"user_tz":300,"elapsed":11,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"3cde750c-3fe1-46d8-e64d-5bf1acf1aa2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'para_index': 0, 'title': 'Myersiohyla liliae', 'doc_id': 'Myersiohyla liliae-0', 'content': 'Myersiohyla liliae is a species of frogs in the family Hylidae. It is endemic to the Pacaraima Mountains in Guyana and known from the region of its type locality in the Kaieteur National Park and from Imbaimadai. The species is dedicated to the daughter of its describer, Lili Kok.', 'page_id': '28259031', 'id': 'Ud-DXIcB1INCf0UyAseC', 'tokens': ['Myersiohyla', 'liliae', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'Hylidae.', 'It', 'is', 'endemic', 'to', 'the', 'Pacaraima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kaieteur', 'National', 'Park', 'and', 'from', 'Imbaimadai.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describer,', 'Lili', 'Kok.'], 'ner_strings': ['B-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner_tags': [5, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]}\n","{'para_index': 0, 'title': 'Hadingus', 'doc_id': 'Hadingus-0', 'content': \"Hadingus was one of the earliest legendary Danish kings according to Saxo Grammaticus' Gesta Danorum, where he has a detailed biography. Georges Dumézil and others have argued that Hadingus was partially modelled on the god Njörðr.\", 'page_id': '4283756', 'id': 'Gy_0WYcB1INCf0UycBhm', 'tokens': ['Hadingus', 'was', 'one', 'of', 'the', 'earliest', 'legendary', 'Danish', 'kings', 'according', 'to', 'Saxo', \"Grammaticus'\", 'Gesta', 'Danorum,', 'where', 'he', 'has', 'a', 'detailed', 'biography.', 'Georges', 'Dumézil', 'and', 'others', 'have', 'argued', 'that', 'Hadingus', 'was', 'partially', 'modelled', 'on', 'the', 'god', 'Njörðr.'], 'ner_strings': ['B-Mythological_king', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Mythological_king', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Mythological_king'], 'ner_tags': [2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 12, 12, 12, 12, 12, 12, 2]}\n"]}],"source":["# If you want you can look at some sample data items\n","print(data_splits[\"train\"][8])\n","print(data_splits[\"dev\"][5])"]},{"cell_type":"markdown","source":["### Data Preprocessing"],"metadata":{"id":"e8QpqReLI5xT"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"utsg41nOizGz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448113174,"user_tz":300,"elapsed":10,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"2d7e8f44-a491-475b-e4d1-54cfd0e23c68"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'para_index': Value(dtype='int64', id=None), 'title': Value(dtype='string', id=None), 'doc_id': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None), 'page_id': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_strings': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n","Original tokens: ['Myersiohyla', 'liliae', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'Hylidae.', 'It', 'is', 'endemic', 'to', 'the', 'Pacaraima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kaieteur', 'National', 'Park', 'and', 'from', 'Imbaimadai.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describer,', 'Lili', 'Kok.']\n","NER labels: [5, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n","Labels: ['B-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","BERT Tokenized:  ['[CLS]', 'Myers', '##io', '##hyl', '##a', 'l', '##ilia', '##e', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'H', '##yl', '##idae', '.', 'It', 'is', 'endemic', 'to', 'the', 'Pac', '##ara', '##ima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kai', '##ete', '##ur', 'National', 'Park', 'and', 'from', 'I', '##mba', '##ima', '##dai', '.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describe', '##r', ',', 'Lil', '##i', 'Ko', '##k', '.', '[SEP]']\n","Vocab size: 28996\n","Token IDs:  [101, 14311, 2660, 18873, 1161, 181, 26502, 1162, 1110, 170, 1530, 1104, 22025, 1107, 1103, 1266, 145, 7777, 5106, 119, 1135, 1110, 6850, 1106, 1103, 19430, 4626, 8628, 5249, 1107, 20345, 1105, 1227, 1121, 1103, 1805, 1104, 1157, 2076, 10157, 1107, 1103, 13354, 16618, 2149, 1305, 1670, 1105, 1121, 146, 10806, 8628, 14117, 119, 1109, 1530, 1110, 3256, 1106, 1103, 1797, 1104, 1157, 5594, 1197, 117, 14138, 1182, 19892, 1377, 119, 102]\n","[None, 0, 0, 0, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 16, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 31, 31, 32, 33, 34, 35, 36, 36, 36, 36, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 46, 46, 47, 47, 48, 48, 48, None]\n"]}],"source":["# This dataset is split into a train, validation and test set, and each token has a label.\n","# Data from the dataset can generally be accessed like a Python dict.\n","print(data_splits['train'].features)\n","\n","# Print the original sentence (which is whitespace tokenized).\n","example_input_tokens = data_splits['train'][8]['tokens']\n","print(f\"Original tokens: {example_input_tokens}\")\n","\n","# Print the labels of the sentence.\n","example_ner_labels = data_splits['train'][8]['ner_tags']\n","print(f\"NER labels: {example_ner_labels}\")\n","\n","# Map integer to string labels for the sentence\n","example_mapped_labels = [labels_int2str[l] for l in example_ner_labels]\n","print(f'Labels: {example_mapped_labels}')\n","\n","# Print the sentence split into tokens.\n","example_tokenized = tokenizer(example_input_tokens, is_split_into_words=True)\n","print('BERT Tokenized: ', example_tokenized.tokens())\n","\n","# Print the number of tokens in the vocabulary\n","print(f'Vocab size: {tokenizer.vocab_size}')\n","\n","# # Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(example_tokenized.tokens()))\n","\n","# Of course, there are now way more tokens than labels! Fortunately the HF tokenizer\n","# provides a function that will give us the mapping:\n","print(example_tokenized.word_ids())"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZCTjD23gHKFB","executionInfo":{"status":"ok","timestamp":1733448113175,"user_tz":300,"elapsed":9,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# We can write a function that uses that along with the original labels to get the new set of labels\n","# for each BERT-tokenized token.\n","def align_labels_with_tokens(labels, word_ids):\n","    new_labels = []\n","    current_word = None\n","    for word_id in word_ids:\n","        if word_id != current_word:\n","            # Start of a new word!\n","            current_word = word_id\n","            label = -100 if word_id is None else labels[word_id]\n","            new_labels.append(label)\n","        elif word_id is None:\n","            # Special token\n","            new_labels.append(-100)\n","        else:\n","            # Same word as previous token\n","            label = labels[word_id]\n","            str_label = labels_int2str[label]\n","            if str_label[0] == 'B':\n","                new_str_label = 'I' + str_label[1:]\n","                label = labels_str2int[new_str_label]\n","            new_labels.append(label)\n","\n","    return new_labels"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zP8KavRk2z9j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448113175,"user_tz":300,"elapsed":9,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"d4251063-59b0-4e52-f651-99e88a281f35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Aligned labels: [-100, 5, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n","Mapped aligned labels: ['_', 'B-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '_']\n"]}],"source":["tokenizer_aligned_labels = align_labels_with_tokens(example_ner_labels, example_tokenized.word_ids())\n","print(f'Aligned labels: {tokenizer_aligned_labels}')\n","print(f'Mapped aligned labels: {[labels_int2str[l] if l >= 0 else \"_\" for l in tokenizer_aligned_labels]}')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"m4cpmrYdHxbS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448113175,"user_tz":300,"elapsed":7,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"33fa58af-27ab-4756-bd22-b3a65c72722d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['[CLS]', 'Myers', '##io', '##hyl', '##a', 'l', '##ilia', '##e', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'H', '##yl', '##idae', '.', 'It', 'is', 'endemic', 'to', 'the', 'Pac', '##ara', '##ima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kai', '##ete', '##ur', 'National', 'Park', 'and', 'from', 'I', '##mba', '##ima', '##dai', '.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describe', '##r', ',', 'Lil', '##i', 'Ko', '##k', '.', '[SEP]']\n","Aligned labels: ['_', 'B-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '_']\n"]}],"source":["# Let's check the function on the example from before. The special tokens don't have labels,\n","# so we'll just replace those with _\n","aligned_labels = align_labels_with_tokens(example_ner_labels, example_tokenized.word_ids())\n","print(f\"Tokens: {example_tokenized.tokens()}\")\n","print(f\"Aligned labels: {[labels_int2str[l] if l >= 0 else '_' for l in aligned_labels]}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"X2qrKgKe7E8N","executionInfo":{"status":"ok","timestamp":1733448113175,"user_tz":300,"elapsed":6,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# Need to get the whole dataset into this format, so need to write a fn\n","# we can apply efficiently across all examples using Dataset.map.\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples[\"tokens\"], truncation=True, is_split_into_words=True\n","    )\n","    all_labels = examples[\"ner_tags\"]\n","    new_labels = []\n","    for i, labels in enumerate(all_labels):\n","        word_ids = tokenized_inputs.word_ids(i)\n","        new_labels.append(align_labels_with_tokens(labels, word_ids))\n","\n","    tokenized_inputs[\"labels\"] = new_labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"GmY5nHOO65kV","executionInfo":{"status":"ok","timestamp":1733448113175,"user_tz":300,"elapsed":6,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# Now we can apply that fn to tokenize all the data\n","tokenized_data_splits = data_splits.map(\n","    tokenize_and_align_labels,\n","    batched=True,\n","    remove_columns=data_splits[\"train\"].column_names,\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"P9gLGKXq6YJO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448113175,"user_tz":300,"elapsed":6,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"1b184c56-b2af-4421-ca7c-c5812ca5f530"},"outputs":[{"output_type":"stream","name":"stdout","text":["Examples:\n","[-100, 9, 4, 4, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 9, 4, 4, 4, 4, 4, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n","[-100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n"]}],"source":["# Testing batcher\n","print(\"Examples:\")\n","for i in range(2):\n","    print(tokenized_data_splits[\"train\"][i][\"labels\"])\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","batch = data_collator([tokenized_data_splits[\"train\"][i] for i in range(2)])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"5GMWXy2o-vu4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448114500,"user_tz":300,"elapsed":1330,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"3fcccaad-990a-4128-81d8-424951438359"},"outputs":[{"output_type":"stream","name":"stdout","text":["['B-Cretaceous_dinosaur', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Cretaceous_dinosaur', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Cretaceous_dinosaur': {'precision': 1.0,\n","  'recall': 0.5,\n","  'f1': 0.6666666666666666,\n","  'number': 2},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 0.5,\n"," 'overall_f1': 0.6666666666666666,\n"," 'overall_accuracy': 0.9904761904761905}"]},"metadata":{},"execution_count":13}],"source":["# Evaluation: we can use the seqeval library to handle calculating span-level precision, recall and F1\n","metric = evaluate.load(\"seqeval\")\n","\n","labels = data_splits[\"train\"][0][\"ner_tags\"]\n","labels = [labels_int2str[i] for i in labels]\n","print(labels)\n","\n","# Make a small change and see how it impacts the score\n","predictions = labels.copy()\n","predictions[0] = \"O\"\n","metric.compute(predictions=[predictions], references=[labels])"]},{"cell_type":"markdown","source":["### Train and evaluation"],"metadata":{"id":"r9hbMjxNJDhD"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"9O7OaUmRoTFl","executionInfo":{"status":"ok","timestamp":1733448114500,"user_tz":300,"elapsed":7,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# This code runs evaluation on test data.\n","# You will need to change this to get it to work for sequence labeling.\n","#\n","# TODO: implement this.\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","@torch.no_grad()\n","def run_eval(model, dataset, batch_size, device, collate_fn=None):\n","    model.eval().to(device)\n","    dataloader = DataLoader(dataset, batch_size, shuffle = False, collate_fn=collate_fn)\n","    loss_fn = nn.NLLLoss(ignore_index=-100)\n","    val_loss_history = []\n","\n","    # Initialize cumulative metrics\n","    total_tokens = 0\n","    correct_predictions = 0\n","    all_predictions = []\n","    all_labels = []\n","\n","    for i, batch in enumerate(dataloader):\n","\n","        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n","        y = batch.pop('labels')\n","\n","        logits = model(**batch)\n","        val_loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))  # Flatten for NLLLoss\n","        val_loss_history.append(val_loss.item())\n","\n","\n","        # Compute predictions\n","        predictions = logits.argmax(dim=-1)  # Get the token-wise predicted class\n","        valid_labels = y.view(-1) != -100  # Mask for valid (non-ignored) tokens\n","\n","        # Update metrics\n","        correct_predictions += (predictions.view(-1)[valid_labels] == y.view(-1)[valid_labels]).sum().item()\n","        total_tokens += valid_labels.sum().item()\n","\n","        # Store for computing precision, recall, and F1\n","        #all_predictions.extend(predictions.view(-1)[valid_labels].tolist())\n","        #all_labels.extend(y.view(-1)[valid_labels].tolist())\n","        all_predictions.extend([labels_int2str[p] for p in predictions.view(-1)[valid_labels].tolist()])\n","        all_labels.extend([labels_int2str[l] for l in y.view(-1)[valid_labels].tolist()])\n","\n","    # Compute overall metrics\n","\n","    val_metric = metric.compute(predictions=[all_predictions], references=[all_labels], zero_division=0)\n","\n","    #overall_accuracy = correct_predictions / total_tokens\n","    #overall_precision = precision_score(all_labels, all_predictions, average=\"macro\", zero_division=0)\n","    #overall_recall = recall_score(all_labels, all_predictions, average=\"macro\", zero_division=0)\n","    #overall_f1 = f1_score(all_labels, all_predictions, average=\"macro\", zero_division=0)\n","\n","    return np.mean(val_loss_history), val_metric"]},{"cell_type":"code","source":["def record_metrics(epoch, train_loss, val_loss, train_acc, val_acc, filename='/content/drive/MyDrive/HW4/log/log_20.csv'):\n","    # Check if the log file already exists\n","    file_exists = os.path.isfile(filename)\n","\n","    # Open the file in append mode; create it if it does not exist\n","    with open(filename, 'a', newline='') as f:\n","        writer = csv.writer(f)\n","\n","        # If the file does not exist, write the header row first\n","        if not file_exists:\n","            writer.writerow(['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])\n","\n","        # Write the current epoch's metrics to the file\n","        writer.writerow([epoch, train_loss, val_loss, train_acc, val_acc])"],"metadata":{"id":"GPOFADgTpwAF","executionInfo":{"status":"ok","timestamp":1733448114500,"user_tz":300,"elapsed":6,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"VQcF7uNCZ7qp","executionInfo":{"status":"ok","timestamp":1733448114500,"user_tz":300,"elapsed":6,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# This code trains the model and evaluates it on test data. It should print\n","# progress messages during training indicating loss, accuracy and training speed.\n","# You will likely need to make changes to this code for it to work for token classification.\n","#\n","# TODO: change this\n","def train(model,\n","          train_dataset,\n","          val_dataset,\n","          num_epochs,\n","          batch_size,\n","          optimizer_cls,\n","          lr,\n","          weight_decay,\n","          device,\n","          collate_fn=None,\n","          log_every=100):\n","    model = model.train().to(device)\n","    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","\n","    # Initialize optimizer\n","    if optimizer_cls == 'SGD':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    elif optimizer_cls == 'Adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    elif optimizer_cls == 'AdamW':\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    # History tracking\n","    train_loss_history = []\n","    train_acc_history = []\n","    val_loss_history = []\n","    val_acc_history = []\n","    best_val_f1 = 0\n","\n","    lossfn = nn.NLLLoss(ignore_index=-100)  # Use ignore_index to handle padding/special tokens\n","\n","    for e in range(num_epochs):\n","        model.train(True)\n","        epoch_loss_history = []\n","        epoch_acc_history = []\n","        start_time = time.time()\n","\n","        for i, batch in enumerate(dataloader):\n","            # Move tensors to the specified device\n","            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n","            labels = batch.pop('labels')\n","\n","            # Forward pass\n","            # For NLLLoss --> expected input ((N, C), (N))\n","            # input:(batch_size, seq_len, num_classes) --> view --> (batch_size*seq_len, num_classes)\n","            # label: (batch_size, seq_len) --> view --> (batch_size*seq_len)\n","\n","            logits = model(**batch)\n","            loss = lossfn(logits.view(-1, logits.size(-1)), labels.view(-1))  # Flatten for token-level loss\n","\n","            # Predictions and accuracy\n","            predictions = logits.argmax(dim=-1)  # Token-level predictions\n","            valid_labels = labels.view(-1) != -100  # Mask to ignore invalid tokens\n","            correct_predictions = (predictions.view(-1)[valid_labels] == labels.view(-1)[valid_labels]).sum().item()\n","            total_valid_tokens = valid_labels.sum().item()\n","            acc = correct_predictions / total_valid_tokens if total_valid_tokens > 0 else 0\n","\n","            # Track loss and accuracy\n","            epoch_loss_history.append(loss.item())\n","            epoch_acc_history.append(acc)\n","\n","            # Logging\n","            if i % log_every == 0:\n","                speed = 0 if i == 0 else log_every / (time.time() - start_time)\n","                print(f'epoch: {e}\\t iter: {i}\\t train_loss: {np.mean(epoch_loss_history):.3e}\\t train_acc:{np.mean(epoch_acc_history):.3f}\\t speed:{speed:.3f} b/s')\n","                start_time = time.time()\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Validation step\n","        val_loss, val_metrics = run_eval(model, val_dataset, batch_size, device, collate_fn=collate_fn)\n","        val_acc = val_metrics['overall_accuracy']\n","        val_p = val_metrics['overall_precision']\n","        val_r = val_metrics['overall_recall']\n","        val_f1 = val_metrics['overall_f1']\n","\n","        #if val_f1 > best_val_f1:\n","        #  best_val_f1 = val_f1\n","        #  model_save_path = \"/content/drive/MyDrive/HW4/model/best_model\"\n","        #  model.save_pretrained(model_save_path)\n","        #  tokenizer.save_pretrained(model_save_path)  # Save the tokenizer too\n","        #  print(f\"Model and tokenizer saved at epoch {e} to {model_save_path}\")\n","\n","\n","        # Append epoch results to history\n","        train_loss_history.append(np.mean(epoch_loss_history))\n","        train_acc_history.append(np.mean(epoch_acc_history))\n","        val_loss_history.append(val_loss)\n","        val_acc_history.append(val_acc)\n","        record_metrics(e, train_loss_history[-1], val_loss_history[-1], train_acc_history[-1], val_acc_history[-1])\n","\n","        # Epoch summary\n","        print(f'epoch: {e}\\t train_loss: {train_loss_history[-1]:.3e}\\t train_accuracy:{train_acc_history[-1]:.3f}\\t val_loss: {val_loss_history[-1]:.3e}\\t val_acc:{val_acc_history[-1]:.3f}\\t val_p:{val_p:.3f}\\t val_r:{val_r:.3f}\\t val_f1:{val_f1:.3f}')\n","\n","    return model, (train_loss_history, train_acc_history, val_loss_history, val_acc_history)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"fb0wjC-zTRz0","executionInfo":{"status":"ok","timestamp":1733448114500,"user_tz":300,"elapsed":6,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"outputs":[],"source":["# This code defines the token classification class using BERT.\n","# The classifier is defined on top of the final layer of BERT.\n","# The classifier has 1 hidden layer with 128 hidden nodes though we have found that\n","# using a smaller number of hidden nodes does not make much difference,\n","#\n","# TODO: implement this\n","class BertForTokenClassification(nn.Module):\n","  def __init__(self, bert_pretrained_config_name, num_classes, freeze_bert=False, dropout_prob=0.1):\n","    '''\n","    BERT with a classification MLP\n","    args:\n","    - bert_pretrained_config_name (str): model name from huggingface hub\n","    - num_classes (int): number of classes in the classification task\n","    - freeze_bert (bool): [default False] If true gradients are not computed for\n","                          BERT's parameters.\n","    - dropout_prob (float): [default 0.1] probability of dropping each activation.\n","    '''\n","    super().__init__()\n","    self.bert = BertModel.from_pretrained(bert_pretrained_config_name)\n","    self.bert.requires_grad_(not freeze_bert)\n","    self.layers = nn.Sequential(\n","      nn.Linear(self.bert.config.hidden_size, 64),\n","      nn.ReLU(),\n","      nn.Dropout(dropout_prob),\n","      nn.Linear(64,32),\n","      nn.ReLU(),\n","      nn.Dropout(dropout_prob),\n","      nn.Linear(32, num_classes),\n","      nn.LogSoftmax(dim=-1)\n","    )\n","  def forward(self, **bert_kwargs):\n","\n","    # Keyword arguments (e.g., input_ids, attention_mask) passed to BERT.\n","    output = self.bert(**bert_kwargs)\n","\n","    # last_hidden_state: The contextual embeddings for each token (shape: (batch_size, seq_len, hidden_size)).\n","    # pooler_output: The pooled embedding for the [CLS] token (shape: (batch_size, hidden_size)).\n","    # For token classification, we need whole hidden state\n","    all_tokens = output.last_hidden_state\n","    logits = self.layers(all_tokens)\n","    return logits\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"w6NVHqYSYds-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733449874950,"user_tz":300,"elapsed":1760456,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"431b37f1-d35e-451d-8935-a770372e79a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num labels: 13\n","Trainable parameters: 108361997\n","\n","epoch: 0\t iter: 0\t train_loss: 2.735e+00\t train_acc:0.007\t speed:0.000 b/s\n","epoch: 0\t train_loss: 1.775e+00\t train_accuracy:0.602\t val_loss: 1.006e+00\t val_acc:0.938\t val_p:0.000\t val_r:0.000\t val_f1:0.000\n","epoch: 1\t iter: 0\t train_loss: 1.234e+00\t train_acc:0.865\t speed:0.000 b/s\n","epoch: 1\t train_loss: 8.519e-01\t train_accuracy:0.911\t val_loss: 4.572e-01\t val_acc:0.938\t val_p:0.000\t val_r:0.000\t val_f1:0.000\n","epoch: 2\t iter: 0\t train_loss: 5.379e-01\t train_acc:0.948\t speed:0.000 b/s\n","epoch: 2\t train_loss: 4.899e-01\t train_accuracy:0.933\t val_loss: 3.155e-01\t val_acc:0.941\t val_p:0.184\t val_r:0.021\t val_f1:0.038\n","epoch: 3\t iter: 0\t train_loss: 3.358e-01\t train_acc:0.946\t speed:0.000 b/s\n","epoch: 3\t train_loss: 3.209e-01\t train_accuracy:0.939\t val_loss: 2.537e-01\t val_acc:0.939\t val_p:0.111\t val_r:0.024\t val_f1:0.040\n","epoch: 4\t iter: 0\t train_loss: 2.214e-01\t train_acc:0.948\t speed:0.000 b/s\n","epoch: 4\t train_loss: 2.143e-01\t train_accuracy:0.949\t val_loss: 2.288e-01\t val_acc:0.944\t val_p:0.147\t val_r:0.143\t val_f1:0.145\n","epoch: 5\t iter: 0\t train_loss: 1.385e-01\t train_acc:0.970\t speed:0.000 b/s\n","epoch: 5\t train_loss: 1.521e-01\t train_accuracy:0.962\t val_loss: 2.273e-01\t val_acc:0.952\t val_p:0.346\t val_r:0.326\t val_f1:0.336\n","epoch: 6\t iter: 0\t train_loss: 1.698e-01\t train_acc:0.955\t speed:0.000 b/s\n","epoch: 6\t train_loss: 1.194e-01\t train_accuracy:0.970\t val_loss: 2.336e-01\t val_acc:0.948\t val_p:0.285\t val_r:0.412\t val_f1:0.337\n","epoch: 7\t iter: 0\t train_loss: 1.370e-01\t train_acc:0.963\t speed:0.000 b/s\n","epoch: 7\t train_loss: 1.007e-01\t train_accuracy:0.974\t val_loss: 2.129e-01\t val_acc:0.955\t val_p:0.352\t val_r:0.409\t val_f1:0.378\n","epoch: 8\t iter: 0\t train_loss: 9.382e-02\t train_acc:0.976\t speed:0.000 b/s\n","epoch: 8\t train_loss: 7.880e-02\t train_accuracy:0.981\t val_loss: 2.270e-01\t val_acc:0.950\t val_p:0.348\t val_r:0.515\t val_f1:0.415\n","epoch: 9\t iter: 0\t train_loss: 5.481e-02\t train_acc:0.988\t speed:0.000 b/s\n","epoch: 9\t train_loss: 6.385e-02\t train_accuracy:0.987\t val_loss: 2.257e-01\t val_acc:0.957\t val_p:0.433\t val_r:0.470\t val_f1:0.450\n","epoch: 10\t iter: 0\t train_loss: 3.703e-02\t train_acc:0.994\t speed:0.000 b/s\n","epoch: 10\t train_loss: 5.231e-02\t train_accuracy:0.990\t val_loss: 2.544e-01\t val_acc:0.949\t val_p:0.404\t val_r:0.558\t val_f1:0.469\n","epoch: 11\t iter: 0\t train_loss: 5.194e-02\t train_acc:0.991\t speed:0.000 b/s\n","epoch: 11\t train_loss: 4.500e-02\t train_accuracy:0.992\t val_loss: 2.763e-01\t val_acc:0.944\t val_p:0.369\t val_r:0.521\t val_f1:0.432\n","epoch: 12\t iter: 0\t train_loss: 3.836e-02\t train_acc:0.994\t speed:0.000 b/s\n","epoch: 12\t train_loss: 4.073e-02\t train_accuracy:0.992\t val_loss: 2.673e-01\t val_acc:0.944\t val_p:0.362\t val_r:0.521\t val_f1:0.427\n","epoch: 13\t iter: 0\t train_loss: 2.646e-02\t train_acc:0.993\t speed:0.000 b/s\n","epoch: 13\t train_loss: 3.476e-02\t train_accuracy:0.994\t val_loss: 2.722e-01\t val_acc:0.948\t val_p:0.404\t val_r:0.546\t val_f1:0.464\n","epoch: 14\t iter: 0\t train_loss: 2.235e-02\t train_acc:0.997\t speed:0.000 b/s\n","epoch: 14\t train_loss: 2.735e-02\t train_accuracy:0.996\t val_loss: 2.833e-01\t val_acc:0.947\t val_p:0.408\t val_r:0.518\t val_f1:0.456\n","\n","Final Loss: 2.860e-01\t Final Accuracy: 0.947\t dev_p:0.408\t dev_r:0.518\t dev_f1:0.456\n"]}],"source":["# This is where fine-tuning of the classifier happens.\n","# Here we are training with batch size 32 for 5 epochs.\n","\n","# At the end of each epoch, you also see validation loss and validation accuracy.\n","# Change the device as described above if you will not be using a GPU\n","\n","# Set the random seed(s) for reproducability\n","torch.random.manual_seed(8942764)\n","torch.cuda.manual_seed(8942764)\n","np.random.seed(8942764)\n","\n","# Make sure this is the same as you use for tokenization!\n","bert_model = 'bert-base-cased'\n","\n","num_labels = len(labels_int2str)\n","print(f\"Num labels: {num_labels}\")\n","\n","# conll hyperparams\n","# multiply your learning rate by k when using batch size of kN\n","lr = 4*2e-5 # 1e-3\n","weight_decay = 0.01\n","epochs = 15\n","batch_size = 32\n","dropout_prob = 0.2\n","freeze_bert = False\n","\n","bert_cls = BertForTokenClassification(bert_model, num_labels, dropout_prob=dropout_prob, freeze_bert=freeze_bert)\n","\n","print(f'Trainable parameters: {sum([p.numel() for p in bert_cls.parameters() if p.requires_grad])}\\n')\n","\n","# Flag for setting \"debug\" mode. Set debug to False for full training.\n","debug = False\n","\n","# Sample a subset of the training data for faster iteration in debug mode\n","subset_size = 1000\n","subset_indices = torch.randperm(len(tokenized_data_splits['train']))[:subset_size]\n","train_subset = Subset(tokenized_data_splits['train'], subset_indices)\n","\n","bert_cls, bert_cls_logs = train(bert_cls, tokenized_data_splits['train'] if not debug else train_subset, tokenized_data_splits['dev'],\n","                                num_epochs=epochs, batch_size=batch_size, optimizer_cls='AdamW',\n","                                lr=lr, weight_decay=weight_decay, device=device,\n","                                collate_fn=data_collator, log_every=10 if debug else 100)\n","\n","# Final eval\n","final_loss, final_metrics = run_eval(bert_cls, tokenized_data_splits['dev'], batch_size=16, device=device, collate_fn=data_collator)\n","final_acc = final_metrics['overall_accuracy']\n","final_p = final_metrics['overall_precision']\n","final_r = final_metrics['overall_recall']\n","final_f1 = final_metrics['overall_f1']\n","print(f'\\nFinal Loss: {final_loss:.3e}\\t Final Accuracy: {final_acc:.3f}\\t dev_p:{final_p:.3f}\\t dev_r:{final_r:.3f}\\t dev_f1:{final_f1:.3f}')\n"]},{"cell_type":"markdown","source":["### Test data"],"metadata":{"id":"aEG5NXfyHoqI"}},{"cell_type":"code","source":["from transformers import BertForTokenClassification\n","# Paths\n","test_data_path = \"/content/drive/MyDrive/HW4/dinos_and_deities_test_bio_nolabels.jsonl\"\n","output_path = \"test_predictions_bert.json\"\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","bert_cls.to(device)\n","bert_cls.eval()"],"metadata":{"id":"4c50OIYoT398","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733450442722,"user_tz":300,"elapsed":1412,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"5e2938e3-b0d9-4419-e608-eb536ecf3f17"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=768, out_features=64, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=64, out_features=32, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.2, inplace=False)\n","    (6): Linear(in_features=32, out_features=13, bias=True)\n","    (7): LogSoftmax(dim=-1)\n","  )\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"PmKlOzVSPv4y","executionInfo":{"status":"ok","timestamp":1733450447953,"user_tz":300,"elapsed":1297,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["import json\n","from datasets import Dataset\n","test_data_path = \"/content/drive/MyDrive/HW4/dinos_and_deities_test_bio_nolabels.jsonl\"\n","with open(test_data_path, \"r\") as f:\n","    test_data = [json.loads(line.strip()) for line in f]\n","\n","# Convert test data into a Hugging Face Dataset\n","test_data = Dataset.from_list(test_data)\n","\n","label_names_fname = \"/content/drive/MyDrive/HW4/dinos_and_deities_train_bio.jsonl.labels\"\n","labels_int2str = []\n","with open(label_names_fname) as f:\n","    labels_int2str = f.read().split()\n","print(f\"Labels: {labels_int2str}\")\n","labels_str2int = {l: i for i, l in enumerate(labels_int2str)}\n","\n","if \"ner_tags\" in test_data.column_names:\n","    test_data = test_data.cast_column(\"ner_tags\", Sequence(ClassLabel(names=labels_int2str)))\n","print(test_data)\n","\n","# Now we can apply that fn to tokenize all the data\n","tokenized_data_test = test_data.map(\n","    tokenize_and_align_labels,\n","    batched=True,\n","    remove_columns=test_data.column_names,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["3b569e382ef74c0cbcb7ed27207a1c51","bd5e07012ea04761b2bda097bccb0d45","c4db2cd7683842e88cebf4b89effeaca","f40a663807344ab79c9603293d5f60e8","85ebc9cd5a184b9f8db1467c662420c1","e73749bc34bd4c06ad386933399ae02a","b0baf43856144182a4b1b61955faab3a","dec9edda25fb4d41b5011085ab8cdfcc","4e49e71e0f9548e095f4e5340939c71d","ba1b523a2d61484bad140e5462c143a7","c4ea8ed9d4b148409ed22b1bbbeb748d","1379965600d14fda9932d34860edb996","428d11759d53411b8396c05f771ed57c","cf9b48959f774818b65e36888a2cb4db","d9e086ee0c5c4dc9ae0585d76f287bf6","104716bc19fa4eb4b824b9874fb5cf6a","e66865b77312455d8f863589ce70f76e","24c51a9d6ddf48f2aebb023b76254c21","0193d8a1ee4a4cdb9f514e54bb8dcf2b","35d8bf11389648c18e1b5b0096514d60","96c5f0722b144083872065848950717a","f61285691c004bc6b9fb1fec0a3e137f"]},"id":"shziWKSFF1F6","executionInfo":{"status":"ok","timestamp":1733450469922,"user_tz":300,"elapsed":1343,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"d97e0def-df86-4312-8ca2-8727cf491639"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Labels: ['I-Aquatic_animal', 'B-Deity', 'B-Mythological_king', 'I-Mythological_king', 'I-Cretaceous_dinosaur', 'B-Aquatic_animal', 'B-Aquatic_mammal', 'I-Goddess', 'I-Deity', 'B-Cretaceous_dinosaur', 'I-Aquatic_mammal', 'B-Goddess', 'O']\n"]},{"output_type":"display_data","data":{"text/plain":["Casting the dataset:   0%|          | 0/303 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b569e382ef74c0cbcb7ed27207a1c51"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['para_index', 'title', 'doc_id', 'content', 'page_id', 'id', 'tokens', 'ner_strings', 'ner_tags'],\n","    num_rows: 303\n","})\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/303 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1379965600d14fda9932d34860edb996"}},"metadata":{}}]},{"cell_type":"code","source":["print(tokenized_data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIOH0X4wiY8z","executionInfo":{"status":"ok","timestamp":1733450474802,"user_tz":300,"elapsed":308,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"d57bf2dd-2eac-40f4-be9b-079851d954b9"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","    num_rows: 303\n","})\n"]}]},{"cell_type":"code","source":["print(tokenized_data_test[\"input_ids\"][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lv7r2fKVwIVo","executionInfo":{"status":"ok","timestamp":1733451397832,"user_tz":300,"elapsed":310,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"be672afb-9e56-4c62-e41a-48204a438b85"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 1130, 1699, 117, 1103, 182, 2744, 2007, 16430, 176, 12640, 25570, 7276, 1162, 113, 3337, 1270, 1202, 5822, 8816, 114, 1110, 2784, 1111, 1103, 1263, 1858, 1920, 1107, 170, 1416, 119, 1188, 12942, 13347, 117, 1972, 117, 1920, 1104, 1103, 8131, 1105, 14780, 1116, 1115, 1202, 1136, 4752, 170, 9131, 117, 1105, 10592, 2019, 170, 9131, 1165, 3238, 119, 1220, 1145, 2812, 1103, 5199, 8131, 1285, 118, 1106, 118, 1285, 113, 1206, 1103, 12104, 172, 18565, 1115, 4752, 1103, 9108, 1104, 170, 9131, 114, 119, 102]\n"]}]},{"cell_type":"code","source":["import json\n","\n","def run_test(model, dataset, batch_size, device, collate_fn=None):\n","    model.eval().to(device)\n","    dataloader = DataLoader(dataset, batch_size, shuffle = False, collate_fn=collate_fn)\n","    loss_fn = nn.NLLLoss(ignore_index=-100)\n","    test_loss_history = []\n","\n","    # Initialize cumulative metrics\n","    total_tokens = 0\n","    correct_predictions = 0\n","    all_predictions_for_compute = []\n","    all_predictions = []\n","    all_labels = []\n","\n","\n","    for i, batch in enumerate(dataloader):\n","\n","        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n","        y = batch.pop('labels')\n","\n","        logits = model(**batch)\n","        test_loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))  # Flatten for NLLLoss\n","        test_loss_history.append(test_loss.item())\n","\n","\n","        # Compute predictions\n","        predictions = logits.argmax(dim=-1)  # Get the token-wise predicted class\n","        print(len(predictions))\n","        test_labels = y.view(-1) != -100  # Mask for valid (non-ignored) tokens\n","\n","        # Update metrics\n","        correct_predictions += (predictions.view(-1)[test_labels] == y.view(-1)[test_labels]).sum().item()\n","        total_tokens += test_labels.sum().item()\n","\n","        # Store for computing precision, recall, and F1\n","        all_predictions.append([labels_int2str[p] for p in predictions.view(-1)[test_labels].tolist()])\n","\n","        #all_labels.extend(y.view(-1)[test_labels].tolist())\n","        all_predictions_for_compute.extend([labels_int2str[p] for p in predictions.view(-1)[test_labels].tolist()])\n","        all_labels.extend([labels_int2str[l] for l in y.view(-1)[test_labels].tolist()])\n","\n","\n","    # Save predictions as JSON\n","    with open(output_path, \"w\") as f:\n","        json.dump(all_predictions, f, indent=4)\n","    print(f\"Predictions saved to {output_path}\")\n","\n","    # Compute overall metrics\n","    test_metric = metric.compute(predictions=[all_predictions_for_compute], references=[all_labels], zero_division=0)\n","\n","    print(\"Test Results:\")\n","    for key, value in test_metric.items():\n","        print(f\"{key}: {value}\")\n","\n","    return np.mean(test_loss_history), test_metric"],"metadata":{"id":"sGIGBJu0d6iQ","executionInfo":{"status":"ok","timestamp":1733451456235,"user_tz":300,"elapsed":1004,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["batch_size = 1"],"metadata":{"id":"p5TbGaP0jHSj","executionInfo":{"status":"ok","timestamp":1733451475560,"user_tz":300,"elapsed":4,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["test_loss, test_metric = run_test(bert_cls, tokenized_data_test, batch_size, device, collate_fn=data_collator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uBaOWExoGXZY","executionInfo":{"status":"ok","timestamp":1733451483762,"user_tz":300,"elapsed":6010,"user":{"displayName":"陳祈曄","userId":"09218473873730549808"}},"outputId":"873629ad-eee5-4723-cd33-70c4e0070ee9"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","Predictions saved to test_predictions_bert.json\n","Test Results:\n","Aquatic_animal: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}\n","Aquatic_mammal: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}\n","Cretaceous_dinosaur: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}\n","Deity: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}\n","Goddess: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}\n","Mythological_king: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}\n","overall_precision: 0.0\n","overall_recall: 0.0\n","overall_f1: 0.0\n","overall_accuracy: 0.9359251259899208\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3b569e382ef74c0cbcb7ed27207a1c51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd5e07012ea04761b2bda097bccb0d45","IPY_MODEL_c4db2cd7683842e88cebf4b89effeaca","IPY_MODEL_f40a663807344ab79c9603293d5f60e8"],"layout":"IPY_MODEL_85ebc9cd5a184b9f8db1467c662420c1"}},"bd5e07012ea04761b2bda097bccb0d45":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e73749bc34bd4c06ad386933399ae02a","placeholder":"​","style":"IPY_MODEL_b0baf43856144182a4b1b61955faab3a","value":"Casting the dataset: 100%"}},"c4db2cd7683842e88cebf4b89effeaca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dec9edda25fb4d41b5011085ab8cdfcc","max":303,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e49e71e0f9548e095f4e5340939c71d","value":303}},"f40a663807344ab79c9603293d5f60e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba1b523a2d61484bad140e5462c143a7","placeholder":"​","style":"IPY_MODEL_c4ea8ed9d4b148409ed22b1bbbeb748d","value":" 303/303 [00:00&lt;00:00, 2520.38 examples/s]"}},"85ebc9cd5a184b9f8db1467c662420c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e73749bc34bd4c06ad386933399ae02a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0baf43856144182a4b1b61955faab3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dec9edda25fb4d41b5011085ab8cdfcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e49e71e0f9548e095f4e5340939c71d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba1b523a2d61484bad140e5462c143a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4ea8ed9d4b148409ed22b1bbbeb748d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1379965600d14fda9932d34860edb996":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_428d11759d53411b8396c05f771ed57c","IPY_MODEL_cf9b48959f774818b65e36888a2cb4db","IPY_MODEL_d9e086ee0c5c4dc9ae0585d76f287bf6"],"layout":"IPY_MODEL_104716bc19fa4eb4b824b9874fb5cf6a"}},"428d11759d53411b8396c05f771ed57c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e66865b77312455d8f863589ce70f76e","placeholder":"​","style":"IPY_MODEL_24c51a9d6ddf48f2aebb023b76254c21","value":"Map: 100%"}},"cf9b48959f774818b65e36888a2cb4db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0193d8a1ee4a4cdb9f514e54bb8dcf2b","max":303,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35d8bf11389648c18e1b5b0096514d60","value":303}},"d9e086ee0c5c4dc9ae0585d76f287bf6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96c5f0722b144083872065848950717a","placeholder":"​","style":"IPY_MODEL_f61285691c004bc6b9fb1fec0a3e137f","value":" 303/303 [00:00&lt;00:00, 701.06 examples/s]"}},"104716bc19fa4eb4b824b9874fb5cf6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e66865b77312455d8f863589ce70f76e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24c51a9d6ddf48f2aebb023b76254c21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0193d8a1ee4a4cdb9f514e54bb8dcf2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35d8bf11389648c18e1b5b0096514d60":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96c5f0722b144083872065848950717a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f61285691c004bc6b9fb1fec0a3e137f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}